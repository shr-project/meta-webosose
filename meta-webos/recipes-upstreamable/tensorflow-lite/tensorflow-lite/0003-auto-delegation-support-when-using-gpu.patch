From d012c17d975bcb077fa842c22603421011f0ae50 Mon Sep 17 00:00:00 2001
From: "yeseul.joo" <yeseul.joo@lge.com>
Date: Tue, 12 Jul 2022 16:45:04 +0900
Subject: [PATCH] auto delegation support when using gpu

:Release Notes:
add tensorflow lite patch to support auto delegation

:Detailed Notes:
support Enable Load Balancing policy for auto delegation library
models that contain dequantize node can also be partitioned for load
balancing

:Testing Performed:
Local Build Test

:QA Notes:
N/A

:Issues Addressed:
N/A
---
 .../lite/delegates/gpu/common/model_builder.cc   |  3 +++
 .../lite/delegates/gpu/common/model_builder.h    |  1 +
 tensorflow/lite/delegates/gpu/delegate.cc        | 16 +++++++++++++++-
 tensorflow/lite/delegates/gpu/delegate.h         |  5 +++++
 tensorflow/lite/delegates/utils.cc               |  9 ++++++---
 tensorflow/lite/delegates/utils.h                |  7 +++++++
 6 files changed, 37 insertions(+), 4 deletions(-)

diff --git a/tensorflow/lite/delegates/gpu/common/model_builder.cc b/tensorflow/lite/delegates/gpu/common/model_builder.cc
index 1f6beafe78a..2494d1eb4d3 100644
--- a/tensorflow/lite/delegates/gpu/common/model_builder.cc
+++ b/tensorflow/lite/delegates/gpu/common/model_builder.cc
@@ -2630,6 +2630,7 @@ std::unique_ptr<TFLiteOperationParser> NewOperationParser(
 // TODO(impjdi): Check ops' parameters.
 TfLiteIntArray* GetOpsToReplace(
     TfLiteContext* context, bool allow_quant_ops, int max_delegated_partitions,
+    int cpu_fallback_percentage,
     const absl::flat_hash_set<TfLiteBuiltinOperator>* excluded_ops) {
   delegates::IsNodeSupportedFn node_supported_fn =
       [=](TfLiteContext* context, TfLiteNode* node,
@@ -2673,6 +2674,8 @@ TfLiteIntArray* GetOpsToReplace(
 
   delegates::FP16GraphPartitionHelper partition_helper(context,
                                                        node_supported_fn);
+  if(cpu_fallback_percentage != 0)
+    partition_helper.SetCpuFallbackPercentage(cpu_fallback_percentage);
   std::set<std::string> unsupported_nodes_info;
   if (partition_helper.Partition(&unsupported_nodes_info) != kTfLiteOk) {
     return TfLiteIntArrayCreate(0);
diff --git a/tensorflow/lite/delegates/gpu/common/model_builder.h b/tensorflow/lite/delegates/gpu/common/model_builder.h
index 7f1c3188044..c4ef2e78c09 100644
--- a/tensorflow/lite/delegates/gpu/common/model_builder.h
+++ b/tensorflow/lite/delegates/gpu/common/model_builder.h
@@ -40,6 +40,7 @@ namespace gpu {
 TfLiteIntArray* GetOpsToReplace(
     TfLiteContext* context, bool allow_quant_ops = false,
     int max_delegated_partitions = 1,
+    int cpu_fallback_percentage = 0,
     const absl::flat_hash_set<TfLiteBuiltinOperator>* excluded_ops = nullptr);
 
 // Extracts TFLite delegate execution plan from the input TFLite context and
diff --git a/tensorflow/lite/delegates/gpu/delegate.cc b/tensorflow/lite/delegates/gpu/delegate.cc
index 9fdb1a88d12..8c72c8b4d84 100644
--- a/tensorflow/lite/delegates/gpu/delegate.cc
+++ b/tensorflow/lite/delegates/gpu/delegate.cc
@@ -103,6 +103,11 @@ class Delegate {
       params.cache_dir = options_.serialization_dir;
       serialization_.reset(new Serialization(params));
     }
+    if (options_.cpu_fallback_percentage < 0) {
+      options_.cpu_fallback_percentage = 0;
+    } else if (options_.cpu_fallback_percentage > 100) {
+      options_.cpu_fallback_percentage = 100;
+    }
   }
 
   TfLiteDelegate* tflite_delegate() { return &delegate_; }
@@ -116,6 +121,7 @@ class Delegate {
   int MaxDelegatedPartitions() const {
     return options_.max_delegated_partitions;
   }
+  int CPUFallbackPercentage() const { return options_.cpu_fallback_percentage; }
   int num_delegate_kernels() const { return num_delegate_kernels_; }
 
  private:
@@ -567,9 +573,16 @@ TfLiteStatus DelegatePrepare(TfLiteContext* context, TfLiteDelegate* delegate) {
     excluded_ops.insert(kTfLiteBuiltinSplit);
     excluded_ops.insert(kTfLiteBuiltinSplitV);
   }
+  if(gpu_delegate->CPUFallbackPercentage() != 0) {
+    TFLITE_LOG_PROD(TFLITE_LOG_INFO,
+      "GPU Load Balancing : %d percent CPU Fallback",
+      gpu_delegate->CPUFallbackPercentage());
+  }
   TfLiteIntArray* ops_to_replace =
       GetOpsToReplace(context, gpu_delegate->IsQuantOpsAllowed(),
-                      gpu_delegate->MaxDelegatedPartitions(), &excluded_ops);
+                      gpu_delegate->MaxDelegatedPartitions(),
+                      gpu_delegate->CPUFallbackPercentage(),
+                      &excluded_ops);
   const auto status = context->ReplaceNodeSubsetsWithDelegateKernels(
       context, kRegistration, ops_to_replace, delegate);
   TFLITE_LOG_PROD(TFLITE_LOG_INFO, "Created %d GPU delegate kernels.",
@@ -595,6 +608,7 @@ TfLiteGpuDelegateOptionsV2 TfLiteGpuDelegateOptionsV2Default() {
   options.max_delegated_partitions = 1;
   options.model_token = nullptr;
   options.serialization_dir = nullptr;
+  options.cpu_fallback_percentage = 0;
   return options;
 }
 
diff --git a/tensorflow/lite/delegates/gpu/delegate.h b/tensorflow/lite/delegates/gpu/delegate.h
index 3a1e1811478..3abb0c5ed2f 100644
--- a/tensorflow/lite/delegates/gpu/delegate.h
+++ b/tensorflow/lite/delegates/gpu/delegate.h
@@ -131,6 +131,11 @@ typedef struct {
   // Set to nullptr in TfLiteGpuDelegateOptionsV2Default(), which implies the
   // delegate will not try serialization.
   const char* model_token;
+
+  // A value between 0 and 100, that represents percentage of nodes that are
+  // forced to run on CPU despite it is supported in GPU.
+  // it's set to 0 in TfLiteGpuDelegateOptionsV2Default().
+  int32_t cpu_fallback_percentage;
 } TfLiteGpuDelegateOptionsV2;
 
 // Populates TfLiteGpuDelegateOptionsV2 as follows:
diff --git a/tensorflow/lite/delegates/utils.cc b/tensorflow/lite/delegates/utils.cc
index 2d56981db73..0697c9b74e8 100644
--- a/tensorflow/lite/delegates/utils.cc
+++ b/tensorflow/lite/delegates/utils.cc
@@ -167,7 +167,7 @@ FP16GraphPartitionHelper::GetNodesOfFirstNLargestPartitionsImpl(
   std::vector<int> ops_to_replace;
 
   if (num_supported_nodes() + constant_dequant_nodes_.size() ==
-      num_total_nodes()) {
+      num_total_nodes() && cpu_fallback_percentage() == 0) {
     // Scenario 1: Full Delegation.
     // We delegate all nodes in this case to avoid unnecessary partitions due to
     // FP16 DEQUANT nodes. This is safe to do since no non-delegated op needs
@@ -186,8 +186,11 @@ FP16GraphPartitionHelper::GetNodesOfFirstNLargestPartitionsImpl(
     if (first_n_partitions.empty()) return ops_to_replace;
     for (int i = 0; i < first_n_partitions.size(); ++i) {
       auto nodes = first_n_partitions[i]->nodes_to_replace;
-      ops_to_replace.insert(ops_to_replace.end(), nodes->data,
-                            nodes->data + nodes->size);
+      auto partition_size = nodes->size
+        - nodes->size * cpu_fallback_percentage() / 100;
+      ops_to_replace.insert(ops_to_replace.end(),
+        nodes->data + nodes->size - partition_size,
+        nodes->data + nodes->size);
     }
   }
 
diff --git a/tensorflow/lite/delegates/utils.h b/tensorflow/lite/delegates/utils.h
index 90dd20e34d9..d06bd12489d 100644
--- a/tensorflow/lite/delegates/utils.h
+++ b/tensorflow/lite/delegates/utils.h
@@ -91,9 +91,14 @@ class GraphPartitionHelper {
     return GetNodesOfFirstNLargestPartitionsImpl(n, min_nodes_per_partition);
   }
 
+  void SetCpuFallbackPercentage(int percentage){ cpu_fallback_percentage_ = percentage; }
+
   int num_total_nodes() const { return num_total_nodes_; }
   int num_supported_nodes() const { return num_supported_nodes_; }
   int num_partitions() const { return partitions_.size(); }
+  int cpu_fallback_percentage() const { return cpu_fallback_percentage_; }
+
+  TfLiteIntArray* GetSupportedNodes() const {return supported_nodes_;}
 
  protected:
   virtual bool IsNodeSupported(TfLiteContext* context, TfLiteNode* node,
@@ -136,6 +141,8 @@ class GraphPartitionHelper {
 
   // Contains an array of supported node indices.
   TfLiteIntArray* supported_nodes_ = nullptr;  // owns the memory
+
+  int cpu_fallback_percentage_ = 0;
 };
 
 // Specialized partitioner for graphs that possibly contain fp16 tensors.
